[TOC]
# SVM模型
它是常见的一种判别方法。在机器学习领域，是一个有监督的学习模型，通常用来进行模式识别、分类以及回归分析。
通过寻求结构化风险最小来提高学习机泛化能力，实现经验风险和置信范围的最小化，从而达到在统计样本量较少的情况下，亦能获得良好统计规律的目的。
它是一种二类分类模型，其基本模型定义为特征空间上的间隔最大的线性分类器，即支持向量机的学习策略便是间隔最大化，最终可转化为一个凸二次规划问题的求解
## 数学基础
### 梯度下降法 

梯度下降法是一种求极值的算法，在对自变量没有限制条件的时候使用。当函数自变量存在取值范围的时候，就无法使用梯度下降法进行求解了。

### 拉格朗日乘子法
适用条件：在自变量有取值范围的函数求极值，一般为等式约束条件
思想：在有限制的情况下，对有限制条件的函数进行转化，转化成无限制条件的函数，使转化完的函数和原函数等价

### KKT条件回顾

## 原理
### 感知机模型
### SVM线性可分、不可分

**SVM线性可分模型** ，即一条直线可以分割两种分类

**SVM线性不可分模型**，两种分类比较紧密，用一条直线无法轻易分割。这种问题的处理方式可以将数据映射到高维，通过一个面来分割两类数据，最后求高维面在低维上的映射。
#### 线性分类
一个线性分类器的学习目标就是要在n维的数据空间中找到一个分类超平面，其方程可以表示为:
![f21abb0d9779f60d114653f30a3a0bb2.png](en-resource://database/543:0)

分类函数：
![50cd0581b52b6b0a04c782ec67c7e93b.png](en-resource://database/545:0)

那如何确定w和b呢？答案是寻找两条边界端或极端划分直线中间的最大间隔（之所以要寻最大间隔是为了能更好的划分不同类的点。
从最大间隔出发（目的本就是为了确定法向量w），转化为求对变量w和b的凸二次规划问题。
#### 线性不可分

## SVM有两大特色：Hinge Loss和Kernel Method

### Hinge Loss
### 核函数

## 




## 和LR比较
### 相同的：

* 分类算法
* 分类决策面是线性的，原始的LR和SVM是线性分类器
* 监督学习算法
* 判别模型

>判别模型会生成一个表示P(Y|X)的判别函数（或预测模型），而生成模型先计算联合概率p(Y,X)然后通过贝叶斯公式转化为条件概率。简单来说，在计算判别模型时，不会计算联合概率，而在计算生成模型时，必须先计算联合概率。或者这样理解：生成算法尝试去找到底这个数据是怎么生成的（产生的），然后再对一个信号进行分类。基于你的生成假设，那么那个类别最有可能产生这个信号，这个信号就属于那个类别。判别模型不关心数据是怎么生成的，它只关心信号之间的差别，然后用差别来简单对给定的一个信号进行分类。常见的判别模型有：KNN、SVM、LR，常见的生成模型有：朴素贝叶斯，隐马尔可夫模型。

### 不同的

1. **本质上**损失函数不同！
逻辑回归的**损失函数**：

![7398ff836be16a017b29e5edb9b2d174.png](en-resource://database/539:0)
支持向量机的**目标函数**：

![07c1df37dda1434442e4285988df30db.png](en-resource://database/541:0)

不同的loss function代表了不同的假设前提，也就代表了不同的分类原理，也就代表了一切！！！
简单来说，逻辑回归方法基于**概率理论**，假设样本为1的概率可以用**sigmoid函数**来表示，然后通过极大似然估计的方法估计出参数的值，具体细节参考http://blog.csdn.net/pakko/article/details/37878837。

支持向量机基于**几何间隔最大化原理**，认为**存在最大几何间隔的分类面为最优分类面**，具体细节参考http://blog.csdn.net/macyang/article/details/38782399

2. 支持向量机只考虑**局部**的边界线附近的点，而逻辑回归考虑**全局**（远离的点对边界线的确定也起作用）

你会发现：影响SVM决策面的样本点只有少数的结构支持向量，当在支持向量外添加或减少任何样本点对分类决策面没有任何影响；而在LR中，每个样本点都会影响决策面的结果。

得知：线性SVM不直接依赖于数据分布，分类平面不受一类点影响；LR则受所有数据点的影响，如果数据不同类别strongly unbalance，一般需要先对数据做balancing。

3.在解决非线性问题时，支持向量机采用**核函数**的机制，而LR通常不采用核函数的方法

分类模型的结果就是计算决策面，模型训练的过程就是决策面的计算过程。通过上面的第二点不同点可以了解，在计算决策面时，SVM算法里只有少数几个代表支持向量的样本参与了计算，也就是只有少数几个样本需要参与核计算（即kernal machine解的系数是稀疏的）。然而，LR算法里，每个样本点都必须参与决策面的计算过程，也就是说，假设我们在LR里也运用核函数的原理，那么每个样本点都必须参与核计算，这带来的计算复杂度是相当高的。所以，在具体应用时，LR很少运用核函数机制。

4.线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响
一个机遇概率，一个机遇距离！

5.结构风险
SVM的损失函数就自带正则！！！（损失函数中的1/2||w||^2项），这就是为什么SVM是**结构风险最小化算法**的原因！！！而LR必须另外在损失函数上添加正则项！！！

结构风险最小化，意思就是在训练误差和模型复杂度之间寻求平衡，防止过拟合，从而达到真实误差的最小化。未达到结构风险最小化的目的，最常用的方法就是添加正则项。

更多详见：
https://www.jianshu.com/p/d50ced9cc80a
https://www.zhihu.com/question/26768865
